{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we can see how to use CUDA alongside Python in the example of adding two vectors. \n",
    "\n",
    "Adding two vectors in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 703 ms, sys: 197 ms, total: 900 ms\n",
      "Wall time: 898 ms\n"
     ]
    }
   ],
   "source": [
    "N = 10000000\n",
    "\n",
    "a = [1. for i in range(N)]\n",
    "b = [2. for i in range(N)]\n",
    "\n",
    "%time c = [a[i] + b[i] for i in range(N)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing speed with numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 ms, sys: 8.09 ms, total: 28.6 ms\n",
      "Wall time: 28.6 ms\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.ones(N)\n",
    "b = np.ones(N) * 2\n",
    "\n",
    "%time c = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way in which we can use CUDA in Python is by using the `numba` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "from numba.cuda import as_cuda_array as ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit \n",
    "def vector_add_kernel(a, b, c, n):\n",
    "    i = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if i < n:\n",
    "        c[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add(a, b):\n",
    "    a = cuda.to_device(a)\n",
    "    b = cuda.to_device(b)\n",
    "    n = a.size\n",
    "    c = np.zeros(n)\n",
    "    c = cuda.to_device(c)\n",
    "    threads_per_block = 128\n",
    "    blocks_per_grid = (n + (threads_per_block - 1)) // threads_per_block\n",
    "    vector_add_kernel[blocks_per_grid, threads_per_block](a, b, c, n)\n",
    "    return c.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 84.6 ms, sys: 31.5 ms, total: 116 ms\n",
      "Wall time: 114 ms\n"
     ]
    }
   ],
   "source": [
    "%time c = vector_add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no much speedup in this case since the overhead of copying the data to the GPU and back to the CPU is high.\n",
    "\n",
    "Another way to use CUDA in Python is by using the `pytorch`:\n",
    "\n",
    "> This requires some extra dependencies `pip install -q wurlitzer ninja`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext wurlitzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cuda(cuda_src, cpp_src, funcs, opt=False, verbose=False):\n",
    "    return load_inline(cuda_sources=[cuda_src], cpp_sources=[cpp_src], functions=funcs,\n",
    "                       extra_cuda_cflags=[\"-O2\"] if opt else [], verbose=verbose, name=\"inline_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_begin = r'''\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void vectorAddKernel(float *out, float *a, float *b, int n) {\n",
    "    int i = threadIdx.x + blockIdx.x * blockDim.x;\n",
    "    if (i < n) {\n",
    "        out[i] = a[i] + b[i];\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor vectorAdd(torch::Tensor a, torch::Tensor b) {\n",
    "    CHECK_INPUT(a); CHECK_INPUT(b);\n",
    "    int n = a.size(0);\n",
    "    auto output = torch::empty(n, a.options());\n",
    "    int threads = 256;\n",
    "    vectorAddKernel<<<cdiv(n, threads), threads>>>(output.data_ptr<float>(), a.data_ptr<float>(), b.data_ptr<float>(), n);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_src = \"torch::Tensor vectorAdd(torch::Tensor a, torch::Tensor b);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/juan/.cache/torch_extensions/py38_cu121 as PyTorch extensions root...\n",
      "The input conditions for extension module inline_ext have changed. Bumping to version 4 and re-building as inline_ext_v4...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/juan/.cache/torch_extensions/py38_cu121/inline_ext/build.ninja...\n",
      "Building extension module inline_ext_v4...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/TH -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/THC -isystem /home/juan/miniconda3/envs/cuda/include -isystem /home/juan/miniconda3/envs/cuda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/juan/.cache/torch_extensions/py38_cu121/inline_ext/main.cpp -o main.o \n",
      "[2/3] /home/juan/miniconda3/envs/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=inline_ext_v4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/TH -isystem /home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/include/THC -isystem /home/juan/miniconda3/envs/cuda/include -isystem /home/juan/miniconda3/envs/cuda/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++17 -c /home/juan/.cache/torch_extensions/py38_cu121/inline_ext/cuda.cu -o cuda.cuda.o \n",
      "[3/3] c++ main.o cuda.cuda.o -shared -L/home/juan/miniconda3/envs/cuda/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/home/juan/miniconda3/envs/cuda/lib -lcudart -o inline_ext_v4.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module inline_ext_v4...\n"
     ]
    }
   ],
   "source": [
    "module = load_cuda(cuda_src, cpp_src, ['vectorAdd'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'vectorAdd']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 ms, sys: 25 Âµs, total: 1.8 ms\n",
      "Wall time: 1.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "a = torch.ones(N, dtype=torch.float32, device='cuda')\n",
    "b = torch.ones(N, dtype=torch.float32, device='cuda') * 2\n",
    "\n",
    "c = module.vectorAdd(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is fast!\n",
    "\n",
    "We can simulate CUDA in Python for easier development and debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kernel(f, times, *args):\n",
    "    for i in range(times): f(i, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_add_kernel(i, out, a, b, n):\n",
    "\tif i < n:\n",
    "\t    out[i] = a[i] + b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.53 s, sys: 30.9 ms, total: 3.56 s\n",
      "Wall time: 3.55 s\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(N)\n",
    "b = np.ones(N) * 2\n",
    "c = np.zeros(N)\n",
    "\n",
    "%time run_kernel(vector_add_kernel, N, c, a, b, N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
